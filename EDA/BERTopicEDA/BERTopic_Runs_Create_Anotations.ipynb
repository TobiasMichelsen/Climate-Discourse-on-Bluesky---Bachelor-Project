{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make annotation sets for 1-4 runs of BERTopic for downstream evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing run1 from: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/BERTopicResult/Cluster_Runs/Run1/run1_result.json\n",
      "Saved 4 annotated clusters to: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/processed/annotated/Sampled_BERT_Runs/cluster_level_annotation_run1.json\n",
      " Processing run2 from: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/BERTopicResult/Cluster_Runs/Run2/run2_result.json\n",
      "Saved 114 annotated clusters to: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/processed/annotated/Sampled_BERT_Runs/cluster_level_annotation_run2.json\n",
      " Processing run3 from: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/BERTopicResult/Cluster_Runs/Run3/run3_result.json\n",
      "Saved 50 annotated clusters to: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/processed/annotated/Sampled_BERT_Runs/cluster_level_annotation_run3.json\n",
      " Processing run4 from: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/BERTopicResult/Cluster_Runs/Run4/run4_result.json\n",
      "Saved 50 annotated clusters to: /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/processed/annotated/Sampled_BERT_Runs/cluster_level_annotation_run4.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Configuration ===\n",
    "BASE_PATH = \"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH\"\n",
    "RUN_FILES = {\n",
    "    \"run1\": f\"{BASE_PATH}/data/BERTopicResult/Cluster_Runs/Run1/run1_result.json\",\n",
    "    \"run2\": f\"{BASE_PATH}/data/BERTopicResult/Cluster_Runs/Run2/run2_result.json\",\n",
    "    \"run3\": f\"{BASE_PATH}/data/BERTopicResult/Cluster_Runs/Run3/run3_result.json\", \n",
    "    \"run4\": f\"{BASE_PATH}/data/BERTopicResult/Cluster_Runs/Run4/run4_result.json\",\n",
    "}\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/processed/annotated/Sampled_BERT_Runs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAMPLES_PER_CLUSTER = 10\n",
    "\n",
    "\n",
    "for run_name, file_path in RUN_FILES.items():\n",
    "    print(f\" Processing {run_name} from: {file_path}\")\n",
    "\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    topic_col = \"topic\" if \"topic\" in df.columns else \"cluster\"\n",
    "    if topic_col not in df.columns or \"text\" not in df.columns:\n",
    "        raise ValueError(f\"Missing required columns in {file_path}. Expected: '{topic_col}', 'text'\")\n",
    "\n",
    "    df[topic_col] = df[topic_col].astype(str)\n",
    "\n",
    "    cluster_samples = []\n",
    "\n",
    "    # Sample 10 posts per cluster\n",
    "    for topic, group in df.groupby(topic_col):\n",
    "        \n",
    "        sample_size = min(SAMPLES_PER_CLUSTER, len(group))\n",
    "        sampled_texts = group.sample(n=sample_size, random_state=42)[\"text\"].tolist()\n",
    "        \n",
    "        cluster_samples.append({\n",
    "            \"topic\": topic,\n",
    "            \"sample_texts\": sampled_texts,\n",
    "            \"annotated_label\": \"\",  # Leave empty for annotation\n",
    "            \"source_run\": run_name\n",
    "        })\n",
    "\n",
    "    output_path = OUTPUT_DIR / f\"cluster_level_annotation_{run_name}.json\"\n",
    "    pd.DataFrame(cluster_samples).to_json(output_path, indent=2, orient=\"records\", force_ascii=False)\n",
    "\n",
    "\n",
    "    print(f\"Saved {len(cluster_samples)} annotated clusters to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No 'yes'-labeled clusters found for run1. Using all clusters.\n",
      "⚠️ No 'yes'-labeled clusters found for run2. Using all clusters.\n",
      "⚠️ No 'yes'-labeled clusters found for run3. Using all clusters.\n",
      "⚠️ No 'yes'-labeled clusters found for run4. Using all clusters.\n",
      "\n",
      "=== BERTopic Entry-Level Filtering Summary ===\n",
      "    Run  Total Entries  Total Clusters  Noise Cluster Size  Removed Entries  \\\n",
      "0  Run1         228275               4               24264                0   \n",
      "1  Run2         182145             114               95358            36664   \n",
      "2  Run3         140161              50               69703            48435   \n",
      "3  Run4          69703              50               49948           118772   \n",
      "\n",
      "   % Removed  \n",
      "0       0.00  \n",
      "1      19.56  \n",
      "2      25.84  \n",
      "3      63.37  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# === Configuration ===\n",
    "BASE_PATH = \"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH\"\n",
    "ANNOTATION_DIR = Path(f\"{BASE_PATH}/data/processed/annotated/Sampled_BERT_Runs\")\n",
    "RESULT_DIR = Path(f\"{BASE_PATH}/data/BERTopicResult/Cluster_Runs\")\n",
    "\n",
    "RUN_NAMES = [\"run1\", \"run2\", \"run3\", \"run4\"]\n",
    "\n",
    "summary = []\n",
    "initial_cids = set()  # Will be filled by run1\n",
    "\n",
    "for i, run in enumerate(RUN_NAMES):\n",
    "    # Load annotation file\n",
    "    annotation_path = ANNOTATION_DIR / f\"cluster_level_annotation_{run}.json\"\n",
    "    if not annotation_path.exists():\n",
    "        print(f\"❌ Missing annotation file: {annotation_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        annotations = pd.DataFrame(json.load(f))\n",
    "\n",
    "    kept_clusters = annotations[annotations[\"annotated_label\"].str.lower() == \"yes\"][\"topic\"].astype(str).tolist()\n",
    "\n",
    "    # Load result file\n",
    "    result_path = RESULT_DIR / run.capitalize() / f\"{run}_result.json\"\n",
    "    try:\n",
    "        df = pd.read_json(result_path, lines=True)\n",
    "        topic_col = \"topic\" if \"topic\" in df.columns else \"cluster\"\n",
    "        df[topic_col] = df[topic_col].astype(str)\n",
    "        \n",
    "        # Filter to only kept clusters\n",
    "        if kept_clusters:\n",
    "            df_filtered = df[df[topic_col].isin(kept_clusters)]\n",
    "        else:\n",
    "            print(f\"⚠️ No 'yes'-labeled clusters found for {run}. Using all clusters.\")\n",
    "            df_filtered = df.copy()\n",
    "\n",
    "        # Track CIDs (or unique post IDs)\n",
    "        current_cids = set(df_filtered[\"cid\"])\n",
    "        if i == 0:\n",
    "            initial_cids = current_cids\n",
    "            removed_entries = 0\n",
    "            removed_pct = 0.0\n",
    "        else:\n",
    "            removed_entries = len(initial_cids - current_cids)\n",
    "            removed_pct = round(removed_entries / len(initial_cids) * 100, 2)\n",
    "\n",
    "        # Add stats\n",
    "        total_entries = len(df_filtered)\n",
    "        num_clusters = df_filtered[topic_col].nunique()\n",
    "        noise_size = (df[topic_col] == \"-1\").sum()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {run}: {e}\")\n",
    "        total_entries = num_clusters = noise_size = removed_entries = removed_pct = \"N/A\"\n",
    "\n",
    "    summary.append({\n",
    "        \"Run\": run.capitalize(),\n",
    "        \"Total Entries\": total_entries,\n",
    "        \"Total Clusters\": num_clusters,\n",
    "        \"Noise Cluster Size\": noise_size,\n",
    "        \"Removed Entries\": removed_entries,\n",
    "        \"% Removed\": removed_pct\n",
    "    })\n",
    "\n",
    "# Display the summary table\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n=== BERTopic Entry-Level Filtering Summary ===\")\n",
    "print(summary_df)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "summary_df.to_csv(f\"{BASE_PATH}/Visualizations/bert_entry_filtering_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
