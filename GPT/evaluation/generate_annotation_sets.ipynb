{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Datasets for Evaluation of LLM: GPT otuput\n",
    "- Dataframe Format: cid, text, assigned, label, annotation, mode, topic\n",
    "\n",
    "- Point: Check whether LLM-assigned labels (Politics, Waste)for individual sampled posts are correct.\n",
    "\n",
    "- 6 different sets: for each mode, set_A, set_B, agreement set (2x3), 400 total\n",
    "\n",
    "- hides assigned_label as not to influence the decision. \n",
    "\n",
    "- Used for further Inter-Annotator Agreement (Kappa score)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation sets generated in: ../../../DS_BachelorProject_PH/data/llm_subtopic/datasets/annotations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "BASE_PATH = Path(\"../../../DS_BachelorProject_PH/data/llm_subtopic/datasets\")\n",
    "\n",
    "MULTI_PATH = BASE_PATH / \"BERTopic_posts_with_labels_multi_label.json\"\n",
    "SINGLE_PATH = BASE_PATH / \"BERTopic_posts_with_labels_single_label.json\"\n",
    "OUT_DIR = BASE_PATH / \"annotations\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "labels_to_sample = [\n",
    "    \"Politics\", \"Renewable\", \"Nature\", \"Activism\", \"Fossil\", \"Waste\", \n",
    "    \"Lifestyle\", \"Weather\", \"Disaster\", \"Agriculture\", \"Transportation\",\n",
    "    \"Electricity\", \"Construction\", \"Climate\", \"Technology\"\n",
    "]\n",
    "\n",
    "samples_per_label = 25\n",
    "agreement_size = 50\n",
    "\n",
    "# --- Load JSON data ---\n",
    "df_multi = pd.read_json(MULTI_PATH)\n",
    "df_single = pd.read_json(SINGLE_PATH)\n",
    "df_multi[\"mode\"] = \"multi_label\"\n",
    "df_single[\"mode\"] = \"single_label\"\n",
    "\n",
    "# --- Helper: stratified sampling by assigned_label ---\n",
    "def stratify_samples(df):\n",
    "    label_buckets = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        lbl = row[\"assigned_label\"]\n",
    "        if lbl in labels_to_sample:\n",
    "            label_buckets[lbl].append(row)\n",
    "\n",
    "    stratified = []\n",
    "    for label, rows in label_buckets.items():\n",
    "        n = min(samples_per_label, len(rows))\n",
    "        stratified.extend(random.sample(rows, n))\n",
    "    return pd.DataFrame(stratified)\n",
    "\n",
    "# --- Stratify datasets ---\n",
    "df_multi_sampled = stratify_samples(df_multi)\n",
    "df_single_sampled = stratify_samples(df_single)\n",
    "\n",
    "# --- Build agreement set (25 from each mode) ---\n",
    "df_agreement_multi = df_multi_sampled.sample(n=agreement_size // 2, random_state=42)\n",
    "df_agreement_single = df_single_sampled.sample(n=agreement_size // 2, random_state=43)\n",
    "\n",
    "# --- Remove from main sets to avoid duplication ---\n",
    "df_multi_sampled = df_multi_sampled.drop(df_agreement_multi.index)\n",
    "df_single_sampled = df_single_sampled.drop(df_agreement_single.index)\n",
    "\n",
    "# --- Split into A/B annotators ---\n",
    "def split_annotators(df, seed):\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    half = len(df) // 2\n",
    "    return df.iloc[:half], df.iloc[half:]\n",
    "\n",
    "multi_A, multi_B = split_annotators(df_multi_sampled, seed=1)\n",
    "single_A, single_B = split_annotators(df_single_sampled, seed=2)\n",
    "\n",
    "# --- Add agreement samples to both annotators ---\n",
    "multi_A = pd.concat([multi_A, df_agreement_multi], ignore_index=True)\n",
    "multi_B = pd.concat([multi_B, df_agreement_multi], ignore_index=True)\n",
    "single_A = pd.concat([single_A, df_agreement_single], ignore_index=True)\n",
    "single_B = pd.concat([single_B, df_agreement_single], ignore_index=True)\n",
    "\n",
    "def format_for_annotation(df, task_type):\n",
    "    df_out = df[[\"cid\", \"text\", \"topic\", \"mode\"]].copy()\n",
    "    df_out[\"annotation\"] = \"\"\n",
    "    \n",
    "    if task_type == \"multi\":\n",
    "        df_out[\"instruction\"] = \"Correct / Partial / Incorrect\"\n",
    "    else:\n",
    "        df_out[\"instruction\"] = \"Correct / Incorrect\"\n",
    "    return df_out\n",
    "\n",
    "# JaySonDerulo \n",
    "format_for_annotation(multi_A, \"multi\").to_json(OUT_DIR / \"multi_label_set_Abel.json\", orient=\"records\",indent=2, force_ascii=False, index=False)\n",
    "format_for_annotation(multi_B, \"multi\").to_json(OUT_DIR / \"multi_label_set_Tobias.json\",orient=\"records\",indent=2, force_ascii=False, index=False)\n",
    "format_for_annotation(df_agreement_multi, \"multi\").to_json(OUT_DIR / \"multi_label_agreement.json\",orient=\"records\",indent=2, force_ascii=False, index=False)\n",
    "\n",
    "format_for_annotation(single_A, \"single\").to_json(OUT_DIR / \"single_label_set_Abel.json\", orient=\"records\",indent=2, force_ascii=False, index=False)\n",
    "format_for_annotation(single_B, \"single\").to_json(OUT_DIR / \"single_label_set_Tobias.json\", index=False, orient=\"records\",indent=2, force_ascii=False,)\n",
    "format_for_annotation(df_agreement_single, \"single\").to_json(OUT_DIR / \"single_label_agreement.json\", orient=\"records\",indent=2, force_ascii=False, index=False)\n",
    "\n",
    "print(\"Annotation sets generated in:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated topic labels in: single_label_set_Abel.json\n",
      "Updated topic labels in: multi_label_set_Tobias.json\n",
      "Updated topic labels in: multi_label_set_Abel.json\n",
      "Updated topic labels in: single_label_agreement.json\n",
      "Updated topic labels in: single_label_set_Tobias.json\n",
      "Updated topic labels in: multi_label_agreement.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ANNOTATION_DIR = Path(\"../../../DS_BachelorProject_PH/data/llm_subtopic/datasets/annotations\")\n",
    "CSV_PATH = Path(\"../../../DS_BachelorProject_PH/data/llm_subtopic/datasets/LLM_Clusters_Topic_Assignment.csv\")\n",
    "\n",
    "# --- Load topic-to-label mapping ---\n",
    "df_map = pd.read_csv(CSV_PATH)\n",
    "topic_to_label = dict(zip(df_map[\"topic\"].astype(str), df_map[\"assigned_label\"]))\n",
    "\n",
    "# --- Process all annotation files ---\n",
    "for json_file in ANNOTATION_DIR.glob(\"*.json\"):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for entry in data:\n",
    "        topic_id = str(entry.get(\"topic\"))\n",
    "        label = topic_to_label.get(topic_id, None)\n",
    "        if label:\n",
    "            entry[\"topic\"] = label\n",
    "        else:\n",
    "            entry[\"topic\"] = \"Unknown\"\n",
    "\n",
    "    # Save back (overwriting the same file or use .with_stem for renaming)\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Updated topic labels in: {json_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Double check:\n",
    "from collections import Counter\n",
    "\n",
    "for file in sorted(ANNOTATION_DIR.glob(\"*.json\")):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    topic_counts = Counter(entry[\"topic\"] for entry in data)\n",
    "    total = len(data)\n",
    "\n",
    "    print(f\"\\ {file.name} â€” {total} entries\")\n",
    "    for label, count in topic_counts.most_common():\n",
    "        print(f\"  {label:<20} {count}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Politics', 38080), ('Renewable', 16828), ('Fossil', 9133), ('Nature', 7886), ('Waste', 4047), ('Transportation', 2996), ('Weather', 2940), ('Activism', 2590), ('Agriculture', 2587), ('Construction', 1247), ('Disaster', 1103), ('Lifestyle', 776)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/llm_subtopic/datasets/BERTopic_posts_with_labels_multi_label.json\")\n",
    "label_counts = Counter(df[\"assigned_label\"])\n",
    "print(label_counts.most_common(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
