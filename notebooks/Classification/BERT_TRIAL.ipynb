{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bertopic in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (0.17.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (1.6.1)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (0.8.40)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (0.5.7)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (4.1.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (6.0.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
      "Requirement already satisfied: packaging in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from plotly>=4.7.0->bertopic) (1.35.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
      "Requirement already satisfied: Pillow in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (10.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.50.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.30.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.12.0)\n",
      "Requirement already satisfied: requests in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
      "Requirement already satisfied: networkx in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.26.20)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bertopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_2.json, current shape: (541, 8)\n",
      "Loaded /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_3.json, current shape: (1168, 8)\n",
      "Loaded /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_8.json, current shape: (1991, 8)\n",
      "Loaded /Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_20.json, current shape: (3021, 8)\n",
      "Using device: cpu\n",
      "\n",
      "Starting embedding with model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 95/95 [00:13<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding done for model 'all-MiniLM-L6-v2' in 13.1s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 50, 'min_samples': 5, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 5, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 models so far.\n",
      "Done | Topics: 5, Outliers: 11 (0.36%) | Coherence: 0.4396 | Diversity: 0.6000 | Time: 9.26s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 50, 'min_samples': 5, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 7, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2 models so far.\n",
      "Done | Topics: 5, Outliers: 7 (0.23%) | Coherence: 0.4396 | Diversity: 0.6000 | Time: 9.1s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 50, 'min_samples': 10, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 5, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 3 models so far.\n",
      "Done | Topics: 5, Outliers: 15 (0.5%) | Coherence: 0.4619 | Diversity: 0.6000 | Time: 8.67s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 50, 'min_samples': 10, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 7, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 4 models so far.\n",
      "Done | Topics: 5, Outliers: 5 (0.17%) | Coherence: 0.4396 | Diversity: 0.6000 | Time: 8.73s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 100, 'min_samples': 5, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 5, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5 models so far.\n",
      "Done | Topics: 3, Outliers: 10 (0.33%) | Coherence: 0.4835 | Diversity: 0.7000 | Time: 8.56s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 100, 'min_samples': 5, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 7, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 6 models so far.\n",
      "Done | Topics: 3, Outliers: 5 (0.17%) | Coherence: 0.5185 | Diversity: 0.7000 | Time: 8.92s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 100, 'min_samples': 10, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 5, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 7 models so far.\n",
      "Done | Topics: 3, Outliers: 10 (0.33%) | Coherence: 0.4835 | Diversity: 0.7000 | Time: 8.71s\n",
      "\n",
      "Running: {'embedding_model': 'all-MiniLM-L6-v2', 'metric': 'cosine', 'min_cluster_size': 100, 'min_samples': 10, 'nr_topics': 15, 'umap_neighbors': 15, 'umap_components': 7, 'umap_min_dist': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 8 models so far.\n",
      "Done | Topics: 3, Outliers: 5 (0.17%) | Coherence: 0.5185 | Diversity: 0.7000 | Time: 9.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 22:26:06,363 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Functions\n",
    "def compute_coherence_score(topic_model, documents, top_n_words=10):\n",
    "    topics = topic_model.get_topics()\n",
    "    topic_words = [\n",
    "        [word for word, _ in topics[topic_id][:top_n_words]]\n",
    "        for topic_id in topics.keys() if topic_id != -1\n",
    "    ]\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words,\n",
    "        texts=tokenized_docs,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def compute_topic_diversity(topic_model, top_n_words=10):\n",
    "    topics = topic_model.get_topics()\n",
    "    topic_words = [\n",
    "        [word for word, _ in topics[topic_id][:top_n_words]]\n",
    "        for topic_id in topics.keys() if topic_id != -1\n",
    "    ]\n",
    "    all_words = [word for words in topic_words for word in words]\n",
    "    unique_words = len(set(all_words))\n",
    "    total_words = len(all_words)\n",
    "    return unique_words / total_words if total_words > 0 else 0\n",
    "\n",
    "# --- DATA ---\n",
    "df_whole = pd.DataFrame()\n",
    "\n",
    "files_to_load = [\n",
    "    \"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_2.json\",\n",
    "    \"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_3.json\",\n",
    "    \"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_8.json\",\n",
    "    \"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/climate_classified/climate_classified_posts_20.json\"\n",
    "]\n",
    "\n",
    "for filepath in files_to_load:\n",
    "    df = pd.read_json(filepath)\n",
    "    df = df[df[\"label\"] == \"yes\"]\n",
    "    df = df[df[\"score\"] >= .99]\n",
    "    df_whole = pd.concat([df_whole, df], ignore_index=True)\n",
    "    print(f\"Loaded {filepath}, current shape: {df_whole.shape}\", flush=True)\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "embedding_models = [\"all-MiniLM-L6-v2\"]\n",
    "\n",
    "min_cluster_sizes = [50, 100]\n",
    "min_samples_vals = [5, 10]\n",
    "distance_metrics = [\"cosine\"]\n",
    "\n",
    "umap_neighbors = [15]\n",
    "umap_components = [5, 7]\n",
    "umap_min_dist = [0.0]\n",
    "\n",
    "nr_topics = 15  # FIXED value outside the loop\n",
    "\n",
    "log_path = os.path.expanduser(\"logs/bertopic_grid_log_local.csv\")\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "\n",
    "log_columns = [\n",
    "    \"embedding_model\", \"metric\", \"min_cluster_size\", \"min_samples\",\n",
    "    \"nr_topics\", \"umap_neighbors\", \"umap_components\", \"umap_min_dist\",\n",
    "    \"n_topics\", \"outliers\", \"outlier_pct\", \"time_sec\", \"coherence\", \"diversity\"\n",
    "]\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    log_df = pd.read_csv(log_path)\n",
    "else:\n",
    "    log_df = pd.DataFrame(columns=log_columns)\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "\n",
    "counter_combinations = len(log_df)\n",
    "max_combinations = 8\n",
    "proportion = counter_combinations / max_combinations\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "texts_to_embed = df_whole[\"text\"].tolist()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\", flush=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "for embed_model in embedding_models:\n",
    "    print(f\"\\nStarting embedding with model: {embed_model}\", flush=True)\n",
    "    model = SentenceTransformer(embed_model, device=device)\n",
    "\n",
    "    start_embed = time.time()\n",
    "    embeddings_local = model.encode(\n",
    "        texts_to_embed,\n",
    "        show_progress_bar=True,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    embed_time = round(time.time() - start_embed, 2)\n",
    "    print(f\"Embedding done for model '{embed_model}' in {embed_time}s\", flush=True)\n",
    "\n",
    "    for (min_cluster_size, min_samples, metric,\n",
    "         n_neighbors, n_components, min_dist) in itertools.product(\n",
    "        min_cluster_sizes, min_samples_vals, distance_metrics,\n",
    "        umap_neighbors, umap_components, umap_min_dist\n",
    "    ):\n",
    "\n",
    "        run_key = {\n",
    "            \"embedding_model\": embed_model,\n",
    "            \"metric\": metric,\n",
    "            \"min_cluster_size\": min_cluster_size,\n",
    "            \"min_samples\": min_samples,\n",
    "            \"nr_topics\": nr_topics,  # fixed\n",
    "            \"umap_neighbors\": n_neighbors,\n",
    "            \"umap_components\": n_components,\n",
    "            \"umap_min_dist\": min_dist\n",
    "        }\n",
    "\n",
    "        existing = log_df[\n",
    "            (log_df.embedding_model == run_key[\"embedding_model\"]) &\n",
    "            (log_df.metric == run_key[\"metric\"]) &\n",
    "            (log_df.min_cluster_size == run_key[\"min_cluster_size\"]) &\n",
    "            (log_df.min_samples == run_key[\"min_samples\"]) &\n",
    "            (log_df.nr_topics == run_key[\"nr_topics\"]) &\n",
    "            (log_df.umap_neighbors == run_key[\"umap_neighbors\"]) &\n",
    "            (log_df.umap_components == run_key[\"umap_components\"]) &\n",
    "            (log_df.umap_min_dist == run_key[\"umap_min_dist\"])\n",
    "        ]\n",
    "\n",
    "        if not existing.empty:\n",
    "            print(f\"Skipping already completed: {run_key}\", flush=True)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nRunning: {run_key}\", flush=True)\n",
    "        start = time.time()\n",
    "\n",
    "        if metric == \"cosine\":\n",
    "            embeddings_used = normalize(embeddings_local, norm=\"l2\")\n",
    "            hdbscan_metric = \"euclidean\"\n",
    "        else:\n",
    "            embeddings_used = embeddings_local\n",
    "            hdbscan_metric = metric\n",
    "\n",
    "        umap_model = UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            n_components=n_components,\n",
    "            min_dist=min_dist,\n",
    "            metric=metric,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric=hdbscan_metric,\n",
    "            cluster_selection_method=\"eom\"\n",
    "        )\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            language=\"english\",\n",
    "            calculate_probabilities=False,\n",
    "            verbose=False,\n",
    "            low_memory=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            topics, _ = topic_model.fit_transform(texts_to_embed, embeddings_used)\n",
    "            df_topics = pd.DataFrame({\n",
    "                \"seq\": df_whole[\"seq\"].tolist(),\n",
    "                \"text\": texts_to_embed,\n",
    "                \"topic\": topics\n",
    "            })\n",
    "\n",
    "            topic_model.reduce_topics(texts_to_embed, nr_topics=nr_topics)\n",
    "\n",
    "            topic_info = topic_model.get_topic_info()\n",
    "            n_topics = len(topic_info[topic_info.Topic != -1])\n",
    "            n_outliers = topic_info[topic_info.Topic == -1].Count.values[0] if -1 in topic_info.Topic.values else 0\n",
    "            n_total = sum(topic_info.Count)\n",
    "            duration = round(time.time() - start, 2)\n",
    "\n",
    "            coherence = compute_coherence_score(topic_model, texts_to_embed)\n",
    "            diversity = compute_topic_diversity(topic_model)\n",
    "\n",
    "            log_entry = {\n",
    "                **run_key,\n",
    "                \"n_topics\": n_topics,\n",
    "                \"outliers\": n_outliers,\n",
    "                \"outlier_pct\": round(n_outliers / n_total * 100, 2),\n",
    "                \"time_sec\": duration,\n",
    "                \"coherence\": round(coherence, 4),\n",
    "                \"diversity\": round(diversity, 4)\n",
    "            }\n",
    "\n",
    "            log_df = pd.concat([log_df, pd.DataFrame([log_entry])], ignore_index=True)\n",
    "            log_df.to_csv(log_path, index=False)\n",
    "\n",
    "            print(f\"Completed {len(log_df)} models so far.\", flush=True)\n",
    "\n",
    "            saved_models.append((coherence + diversity, topic_model, run_key))\n",
    "            saved_models = sorted(saved_models, key=lambda x: x[0], reverse=True)[:1]\n",
    "\n",
    "            counter_combinations += 1\n",
    "            proportion = counter_combinations / max_combinations\n",
    "\n",
    "            if max_combinations >= 10 and counter_combinations % (max_combinations // 10) == 0:\n",
    "                print(f\"Progress: {counter_combinations}/{max_combinations} models completed ({int(proportion * 100)}%)\", flush=True)\n",
    "\n",
    "            print(f\"Done | Topics: {n_topics}, Outliers: {n_outliers} ({log_entry['outlier_pct']}%) | Coherence: {coherence:.4f} | Diversity: {diversity:.4f} | Time: {duration}s\", flush=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for config: {run_key} — {e}\", flush=True)\n",
    "\n",
    "        finally:\n",
    "            del topic_model\n",
    "            gc.collect()\n",
    "\n",
    "# --- SAVE BEST MODEL ---\n",
    "for idx, (score, model, params) in enumerate(saved_models):\n",
    "    save_dir = f\"logs/top_models/model_{idx+1}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.save(os.path.join(save_dir, \"model\"))\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"params.json\"), \"w\") as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    \n",
    "    df_topics = pd.DataFrame({\n",
    "        \"seq\": df_whole[\"seq\"].tolist(),\n",
    "        \"text\": texts_to_embed,\n",
    "        \"topic\": model.topics_\n",
    "    })\n",
    "    df_topics.to_json(os.path.join(save_dir, \"topics_with_seq.json\"), orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "model = BERTopic.load(\"logs/top_models/model_1/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>-1_local_weather_pi_raspberry</td>\n",
       "      <td>[local, weather, pi, raspberry, kampot, cambod...</td>\n",
       "      <td>[The local weather in Kampot, Cambodia from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2669</td>\n",
       "      <td>0_the_to_and_of</td>\n",
       "      <td>[the, to, and, of, is, in, for, that, on, it]</td>\n",
       "      <td>[Now is not the time to vote Green either in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>238</td>\n",
       "      <td>1_apr_snow_low_precip</td>\n",
       "      <td>[apr, snow, low, precip, high, 00, missing, cl...</td>\n",
       "      <td>[BURNS OR Apr 6 Climate Report High 69 Low 26 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>2_apr_airport_snow_precip</td>\n",
       "      <td>[apr, airport, snow, precip, 00, low, high, mi...</td>\n",
       "      <td>[MADERA CA AIRPORT Apr 6 Climate Report High 7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                           Name  \\\n",
       "0     -1      5  -1_local_weather_pi_raspberry   \n",
       "1      0   2669                0_the_to_and_of   \n",
       "2      1    238          1_apr_snow_low_precip   \n",
       "3      2    109      2_apr_airport_snow_precip   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [local, weather, pi, raspberry, kampot, cambod...   \n",
       "1      [the, to, and, of, is, in, for, that, on, it]   \n",
       "2  [apr, snow, low, precip, high, 00, missing, cl...   \n",
       "3  [apr, airport, snow, precip, 00, low, high, mi...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [The local weather in Kampot, Cambodia from a ...  \n",
       "1  [Now is not the time to vote Green either in c...  \n",
       "2  [BURNS OR Apr 6 Climate Report High 69 Low 26 ...  \n",
       "3  [MADERA CA AIRPORT Apr 6 Climate Report High 7...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See basic topic information\n",
    "model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
