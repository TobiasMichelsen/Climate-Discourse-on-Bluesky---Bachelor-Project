{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic: \n",
    "- Preprocess (normalize text, filter for \"yes\" label)\n",
    "\n",
    "- Embedding (convert text to number representation)\n",
    "\n",
    "- Top Modeling (find different cluster setups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports / installs\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import hdbscan\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    # Remove non-letter characters (keep punctuation if needed)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def generate_embeddings(texts, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"../../data/climate_classified/\" #Currently uses the 14 jsons * 100,000 posts \n",
    "\n",
    "json_pattern = os.path.join(INPUT_PATH,'*.json')\n",
    "combined_paths = glob.glob(json_pattern)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for path in combined_paths:\n",
    "    try:\n",
    "        df = pd.read_json(path)\n",
    "        dfs.append(df)\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to read {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1311270 posts from 14 files.\n"
     ]
    }
   ],
   "source": [
    "if dfs:\n",
    "    temp = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Loaded {len(temp)} posts from {len(dfs)} files.\")\n",
    "else:\n",
    "    print(\"No data loaded.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Currently filters for 60+ characters and score >= 0.99 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining posts after full filtering: 11467\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter for climate-related posts only\n",
    "climate_df = temp[temp[\"label\"] == \"yes\"].copy()\n",
    "\n",
    "# Step 2: Light preprocessing\n",
    "climate_df[\"clean_text\"] = climate_df[\"text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Step 3: Filter on character length and score\n",
    "climate_df = climate_df[\n",
    "    \n",
    "    (climate_df[\"clean_text\"].str.len() >= 60) &\n",
    "    (climate_df[\"score\"] >= 0.99)\n",
    "].copy()\n",
    "\n",
    "print(f\"Remaining posts after full filtering: {len(climate_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved to: ../../data/filtered/above60chars_above99score.json\n"
     ]
    }
   ],
   "source": [
    "# Save filtered climate_df\n",
    "\n",
    "output_path = \"../../data/filtered/above60chars_above99score.json\"\n",
    "climate_df.to_json(output_path)\n",
    "\n",
    "print(f\"Filtered dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['repo', 'seq', 'text', 'timestamp', 'cid', 'uri', 'label', 'score',\n",
       "       'clean_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>seq</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cid</th>\n",
       "      <th>uri</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>did:plc:uli2rqyfqasvuawksu2z5jkc</td>\n",
       "      <td>7778280581</td>\n",
       "      <td>Trump's executive order trying to block state ...</td>\n",
       "      <td>2025-04-09 21:10:45.855</td>\n",
       "      <td>bafyreihbjn7mnkbiytl4wc2jjhukux7xfncg772auwhhu...</td>\n",
       "      <td>at://did:plc:uli2rqyfqasvuawksu2z5jkc/app.bsky...</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.997684</td>\n",
       "      <td>trumps executive order trying to block state c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>did:plc:4zh2idecxr5zudhn3oniodhw</td>\n",
       "      <td>7778286641</td>\n",
       "      <td>Spain and Canada signed agreements on renewabl...</td>\n",
       "      <td>2025-04-09 21:10:53.664</td>\n",
       "      <td>bafyreibvwoj6qzbffnpz4rkgxena26ejvpfqoznkbed7n...</td>\n",
       "      <td>at://did:plc:4zh2idecxr5zudhn3oniodhw/app.bsky...</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.993054</td>\n",
       "      <td>spain and canada signed agreements on renewabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>did:plc:m6ntt433rso3lp7dxaja3mue</td>\n",
       "      <td>7778293323</td>\n",
       "      <td>When did you bitch about what Republicans were...</td>\n",
       "      <td>2025-04-09 21:11:03.083</td>\n",
       "      <td>bafyreic4vipeqxz6mlm36qa7uw3qqjbxwo3qtvucl2fek...</td>\n",
       "      <td>at://did:plc:m6ntt433rso3lp7dxaja3mue/app.bsky...</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.996543</td>\n",
       "      <td>when did you bitch about what republicans were...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>did:plc:cm4nhw2xk43bczonk7mbfvrb</td>\n",
       "      <td>7778294643</td>\n",
       "      <td>Hydrogen, as you know, is useful for decarboni...</td>\n",
       "      <td>2025-04-09 21:11:05.110</td>\n",
       "      <td>bafyreidsl6kwy2dx6rtvqkhjyafcffv6cwdud7aefrubf...</td>\n",
       "      <td>at://did:plc:cm4nhw2xk43bczonk7mbfvrb/app.bsky...</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.997304</td>\n",
       "      <td>hydrogen as you know is useful for decarbonisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>did:plc:ci5fsjcdjgoct5k3yllky4ud</td>\n",
       "      <td>7778294887</td>\n",
       "      <td>Either we end the Fossil Fuel Era or the Fossi...</td>\n",
       "      <td>2025-04-09 21:11:05.321</td>\n",
       "      <td>bafyreifegyoen4hku664cni3qqh3v6xbnoptueyb76dqm...</td>\n",
       "      <td>at://did:plc:ci5fsjcdjgoct5k3yllky4ud/app.bsky...</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.995790</td>\n",
       "      <td>either we end the fossil fuel era or the fossi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 repo         seq  \\\n",
       "25   did:plc:uli2rqyfqasvuawksu2z5jkc  7778280581   \n",
       "204  did:plc:4zh2idecxr5zudhn3oniodhw  7778286641   \n",
       "411  did:plc:m6ntt433rso3lp7dxaja3mue  7778293323   \n",
       "441  did:plc:cm4nhw2xk43bczonk7mbfvrb  7778294643   \n",
       "448  did:plc:ci5fsjcdjgoct5k3yllky4ud  7778294887   \n",
       "\n",
       "                                                  text  \\\n",
       "25   Trump's executive order trying to block state ...   \n",
       "204  Spain and Canada signed agreements on renewabl...   \n",
       "411  When did you bitch about what Republicans were...   \n",
       "441  Hydrogen, as you know, is useful for decarboni...   \n",
       "448  Either we end the Fossil Fuel Era or the Fossi...   \n",
       "\n",
       "                  timestamp  \\\n",
       "25  2025-04-09 21:10:45.855   \n",
       "204 2025-04-09 21:10:53.664   \n",
       "411 2025-04-09 21:11:03.083   \n",
       "441 2025-04-09 21:11:05.110   \n",
       "448 2025-04-09 21:11:05.321   \n",
       "\n",
       "                                                   cid  \\\n",
       "25   bafyreihbjn7mnkbiytl4wc2jjhukux7xfncg772auwhhu...   \n",
       "204  bafyreibvwoj6qzbffnpz4rkgxena26ejvpfqoznkbed7n...   \n",
       "411  bafyreic4vipeqxz6mlm36qa7uw3qqjbxwo3qtvucl2fek...   \n",
       "441  bafyreidsl6kwy2dx6rtvqkhjyafcffv6cwdud7aefrubf...   \n",
       "448  bafyreifegyoen4hku664cni3qqh3v6xbnoptueyb76dqm...   \n",
       "\n",
       "                                                   uri label     score  \\\n",
       "25   at://did:plc:uli2rqyfqasvuawksu2z5jkc/app.bsky...   yes  0.997684   \n",
       "204  at://did:plc:4zh2idecxr5zudhn3oniodhw/app.bsky...   yes  0.993054   \n",
       "411  at://did:plc:m6ntt433rso3lp7dxaja3mue/app.bsky...   yes  0.996543   \n",
       "441  at://did:plc:cm4nhw2xk43bczonk7mbfvrb/app.bsky...   yes  0.997304   \n",
       "448  at://did:plc:ci5fsjcdjgoct5k3yllky4ud/app.bsky...   yes  0.995790   \n",
       "\n",
       "                                            clean_text  \n",
       "25   trumps executive order trying to block state c...  \n",
       "204  spain and canada signed agreements on renewabl...  \n",
       "411  when did you bitch about what republicans were...  \n",
       "441  hydrogen as you know is useful for decarbonisi...  \n",
       "448  either we end the fossil fuel era or the fossi...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Embedding Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -U sentence-transforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 359/359 [01:47<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-miniLM-L6-v2')\n",
    "texts_to_embed = climate_df[\"clean_text\"].tolist()\n",
    "embeddings = model.encode(texts_to_embed, show_progress_bar=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
      "\u001b[K     |████████████████████████████████| 150 kB 261 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly>=4.7.0\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 167 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (1.6.1)\n",
      "Collecting hdbscan>=0.8.29\n",
      "  Downloading hdbscan-0.8.40.tar.gz (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 60 kB/s eta 0:00:016\n",
      "\u001b[?25h\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/f3/z043mppd07z6npdj2ch2t1mm0000gn/T/pip-build-env-ljns71ow/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/f3/z043mppd07z6npdj2ch2t1mm0000gn/T/pip-build-env-ljns71ow/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/f3/z043mppd07z6npdj2ch2t1mm0000gn/T/pip-build-env-ljns71ow/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/f3/z043mppd07z6npdj2ch2t1mm0000gn/T/pip-build-env-ljns71ow/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/f3/z043mppd07z6npdj2ch2t1mm0000gn/T/pip-build-env-ljns71ow/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/f3/z043mppd07z6npdj2ch2t1mm0000gn/T/pip-build-env-ljns71ow/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (4.1.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from bertopic) (2.2.3)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 201 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
      "Requirement already satisfied: packaging in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
      "Collecting narwhals>=1.15.1\n",
      "  Downloading narwhals-1.35.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 31 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.50.3)\n",
      "Requirement already satisfied: Pillow in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.30.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.12.0)\n",
      "Requirement already satisfied: requests in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 49 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting numba>=0.51.2\n",
      "  Downloading numba-0.60.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 411 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.8 MB 201 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.1.31)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.40-cp39-cp39-macosx_10_9_universal2.whl size=1496043 sha256=6f58e218ccc9820942fa3f04ba5241d6cd07898c72c37e4e137c8d52c25e1a2f\n",
      "  Stored in directory: /Users/tobiasmichelsen/Library/Caches/pip/wheels/d7/a1/fa/cf52ce36f64d96efeaea8fcebb37b11c9f27308a914381ff0e\n",
      "Successfully built hdbscan\n",
      "Installing collected packages: llvmlite, numba, pynndescent, narwhals, umap-learn, plotly, hdbscan, bertopic\n",
      "Successfully installed bertopic-0.17.0 hdbscan-0.8.40 llvmlite-0.43.0 narwhals-1.35.0 numba-0.60.0 plotly-6.0.1 pynndescent-0.5.13 umap-learn-0.5.7\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(language=\"english\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 19:25:05,472 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-19 19:25:10,167 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-19 19:25:10,169 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-19 19:25:10,672 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-19 19:25:10,682 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-19 19:25:11,104 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4920</td>\n",
       "      <td>-1_the_and_to_of</td>\n",
       "      <td>[the, and, to, of, in, for, is, on, that, we]</td>\n",
       "      <td>[understandable i know a lot of people essenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>326</td>\n",
       "      <td>0_coal_plants_trump_clean</td>\n",
       "      <td>[coal, plants, trump, clean, mine, beautiful, ...</td>\n",
       "      <td>[trump tried to bring back coal in his first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>283</td>\n",
       "      <td>1_depth_snow_apr_precip</td>\n",
       "      <td>[depth, snow, apr, precip, low, high, iembot, ...</td>\n",
       "      <td>[chanhassen mn apr climate report high low pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>2_solar_electricity_panels_power</td>\n",
       "      <td>[solar, electricity, panels, power, generation...</td>\n",
       "      <td>[renewables met of the growth in electricity d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>269</td>\n",
       "      <td>3_change_climate_denial_they</td>\n",
       "      <td>[change, climate, denial, they, it, you, about...</td>\n",
       "      <td>[i think a lot about misinformation and how we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>246</td>\n",
       "      <td>4_apr_missing_iembot_additional</td>\n",
       "      <td>[apr, missing, iembot, additional, details, vi...</td>\n",
       "      <td>[atlanta apr climate report high low precip sn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>146</td>\n",
       "      <td>5_forests_trees_logging_forest</td>\n",
       "      <td>[forests, trees, logging, forest, national, tr...</td>\n",
       "      <td>[logging doesnt prevent wildfires but trump is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>138</td>\n",
       "      <td>6_ai_use_artists_crypto</td>\n",
       "      <td>[ai, use, artists, crypto, environmental, ener...</td>\n",
       "      <td>[that is a good thing more ai more coal and ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>7_laws_order_state_states</td>\n",
       "      <td>[laws, order, state, states, executive, trump,...</td>\n",
       "      <td>[in a sweeping executive order signed late tue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>104</td>\n",
       "      <td>8_airport_apr_missing_iembot</td>\n",
       "      <td>[airport, apr, missing, iembot, additional, de...</td>\n",
       "      <td>[fullerton airport ca apr climate report high ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "      <td>9_he_his_hes_trump</td>\n",
       "      <td>[he, his, hes, trump, him, change, climate, th...</td>\n",
       "      <td>[doesnt he have retaining walls at his scotlan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>10_game_art_artist_portfolioday</td>\n",
       "      <td>[game, art, artist, portfolioday, commissions,...</td>\n",
       "      <td>[the orange experiment part thread megaman roc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>96</td>\n",
       "      <td>11_plastic_plastics_microplastics_pollution</td>\n",
       "      <td>[plastic, plastics, microplastics, pollution, ...</td>\n",
       "      <td>[dr sarmad latif founder of poke plastic is le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>96</td>\n",
       "      <td>12_bskyappstarterpack_climate_join_series</td>\n",
       "      <td>[bskyappstarterpack, climate, join, series, he...</td>\n",
       "      <td>[this award highlights the importance of furth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>92</td>\n",
       "      <td>13_tariffs_trade_tariff_trumps</td>\n",
       "      <td>[tariffs, trade, tariff, trumps, trump, us, en...</td>\n",
       "      <td>[it is not in our best interest to put high ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                         Name  \\\n",
       "0      -1   4920                             -1_the_and_to_of   \n",
       "1       0    326                    0_coal_plants_trump_clean   \n",
       "2       1    283                      1_depth_snow_apr_precip   \n",
       "3       2    271             2_solar_electricity_panels_power   \n",
       "4       3    269                 3_change_climate_denial_they   \n",
       "5       4    246              4_apr_missing_iembot_additional   \n",
       "6       5    146               5_forests_trees_logging_forest   \n",
       "7       6    138                      6_ai_use_artists_crypto   \n",
       "8       7    107                    7_laws_order_state_states   \n",
       "9       8    104                 8_airport_apr_missing_iembot   \n",
       "10      9    103                           9_he_his_hes_trump   \n",
       "11     10    101              10_game_art_artist_portfolioday   \n",
       "12     11     96  11_plastic_plastics_microplastics_pollution   \n",
       "13     12     96    12_bskyappstarterpack_climate_join_series   \n",
       "14     13     92               13_tariffs_trade_tariff_trumps   \n",
       "\n",
       "                                       Representation  \\\n",
       "0       [the, and, to, of, in, for, is, on, that, we]   \n",
       "1   [coal, plants, trump, clean, mine, beautiful, ...   \n",
       "2   [depth, snow, apr, precip, low, high, iembot, ...   \n",
       "3   [solar, electricity, panels, power, generation...   \n",
       "4   [change, climate, denial, they, it, you, about...   \n",
       "5   [apr, missing, iembot, additional, details, vi...   \n",
       "6   [forests, trees, logging, forest, national, tr...   \n",
       "7   [ai, use, artists, crypto, environmental, ener...   \n",
       "8   [laws, order, state, states, executive, trump,...   \n",
       "9   [airport, apr, missing, iembot, additional, de...   \n",
       "10  [he, his, hes, trump, him, change, climate, th...   \n",
       "11  [game, art, artist, portfolioday, commissions,...   \n",
       "12  [plastic, plastics, microplastics, pollution, ...   \n",
       "13  [bskyappstarterpack, climate, join, series, he...   \n",
       "14  [tariffs, trade, tariff, trumps, trump, us, en...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [understandable i know a lot of people essenti...  \n",
       "1   [trump tried to bring back coal in his first t...  \n",
       "2   [chanhassen mn apr climate report high low pre...  \n",
       "3   [renewables met of the growth in electricity d...  \n",
       "4   [i think a lot about misinformation and how we...  \n",
       "5   [atlanta apr climate report high low precip sn...  \n",
       "6   [logging doesnt prevent wildfires but trump is...  \n",
       "7   [that is a good thing more ai more coal and ga...  \n",
       "8   [in a sweeping executive order signed late tue...  \n",
       "9   [fullerton airport ca apr climate report high ...  \n",
       "10  [doesnt he have retaining walls at his scotlan...  \n",
       "11  [the orange experiment part thread megaman roc...  \n",
       "12  [dr sarmad latif founder of poke plastic is le...  \n",
       "13  [this award highlights the importance of furth...  \n",
       "14  [it is not in our best interest to put high ta...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(texts_to_embed, embeddings)\n",
    "\n",
    "topic_model.get_topic_info().head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "To improve the clustering of topics, we can improve the model in several ways:\n",
    "\n",
    "- Change UDBSCAN settings (min_cluster_size, min_samples, metrics = euclidean, manhattan, cosine)\n",
    "- Manual merging of topics (Two related fine-grained topics could be merged into a broader, more general topic)\n",
    "- Change Sentence Transformer to a different model (ie. \"all-mpnet-base-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HDBSCAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(embed_model)\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m min_cluster_size, min_samples, nr_topics \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mproduct(min_cluster_sizes, min_samples_vals, nr_topics_vals):\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m     \u001b[39m# HDBSCAN config\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     hdbscan_model \u001b[39m=\u001b[39m HDBSCAN(\n\u001b[1;32m     17\u001b[0m         min_cluster_size\u001b[39m=\u001b[39mmin_cluster_size,\n\u001b[1;32m     18\u001b[0m         min_samples\u001b[39m=\u001b[39mmin_samples,\n\u001b[1;32m     19\u001b[0m         metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m         cluster_selection_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meom\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[39m# BERTopic model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     topic_model \u001b[39m=\u001b[39m BERTopic(\n\u001b[1;32m     25\u001b[0m         hdbscan_model\u001b[39m=\u001b[39mhdbscan_model,\n\u001b[1;32m     26\u001b[0m         language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m         calculate_probabilities\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HDBSCAN' is not defined"
     ]
    }
   ],
   "source": [
    "# Define search grid\n",
    "min_cluster_sizes = [10, 30]\n",
    "min_samples_vals = [5, 10]\n",
    "nr_topics_vals = [None, 30]\n",
    "embedding_models = [\"all-MiniLM-L6-v2\",\"all-mpnet-base-v2\"]  # You can add more if needed\n",
    "\n",
    "# Storage\n",
    "results = []\n",
    "\n",
    "for embed_model in embedding_models:\n",
    "    model = SentenceTransformer(embed_model)\n",
    "\n",
    "    for min_cluster_size, min_samples, nr_topics in itertools.product(min_cluster_sizes, min_samples_vals, nr_topics_vals):\n",
    "\n",
    "        # HDBSCAN config\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean',\n",
    "            cluster_selection_method='eom'\n",
    "        )\n",
    "\n",
    "        # BERTopic model\n",
    "        topic_model = BERTopic(\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            language=\"english\",\n",
    "            calculate_probabilities=False,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Fit model\n",
    "        topics, _ = topic_model.fit_transform(texts_to_embed, embeddings)\n",
    "\n",
    "        # Reduce topics if requested\n",
    "        if nr_topics:\n",
    "            topic_model.reduce_topics(texts_to_embed, nr_topics=nr_topics)\n",
    "            topics = topic_model.topics_\n",
    "\n",
    "        # Evaluate\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        n_topics = len(topic_info[topic_info.Topic != -1])\n",
    "        n_outliers = topic_info[topic_info.Topic == -1].Count.values[0] if -1 in topic_info.Topic.values else 0\n",
    "        n_total = sum(topic_info.Count)\n",
    "\n",
    "        results.append({\n",
    "            \"embedding_model\": embed_model,\n",
    "            \"min_cluster_size\": min_cluster_size,\n",
    "            \"min_samples\": min_samples,\n",
    "            \"nr_topics\": nr_topics if nr_topics else \"None\",\n",
    "            \"n_topics\": n_topics,\n",
    "            \"outliers\": n_outliers,\n",
    "            \"outlier_pct\": round(n_outliers / n_total * 100, 2),\n",
    "        })\n",
    "\n",
    "        print(f\"Finished: min_cluster_size={min_cluster_size}, min_samples={min_samples}, nr_topics={nr_topics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
