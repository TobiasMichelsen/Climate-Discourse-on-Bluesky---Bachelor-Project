{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Climate | Non-Climate posts using Word Embeddings + Cosine Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "\u001b[K     |████████████████████████████████| 340 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.6.0-cp39-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 66.5 MB 107 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.5.0 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.12.2)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "\u001b[K     |████████████████████████████████| 481 kB 19.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (10.3.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: requests in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 21.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 27.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[K     |████████████████████████████████| 418 kB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tobiasmichelsen/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Installing collected packages: tqdm, pyyaml, fsspec, filelock, mpmath, MarkupSafe, huggingface-hub, tokenizers, threadpoolctl, sympy, scipy, safetensors, regex, joblib, jinja2, transformers, torch, scikit-learn, sentence-transformers\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/Users/tobiasmichelsen/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/Users/tobiasmichelsen/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/Users/tobiasmichelsen/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/Users/tobiasmichelsen/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/Users/tobiasmichelsen/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.1 jinja2-3.1.6 joblib-1.4.2 mpmath-1.3.0 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.13.1 sentence-transformers-4.0.1 sympy-1.13.1 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.50.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/filtered/english_posts.json\") as f:\n",
    "    english_posts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extend these prompts to capture the \"true\" meaning of climate from multiple perspectives. \n",
    "\n",
    "#small issue with a cosine similarity model for classifying whether or not a given text belongs to [Climate] or [Non-Climate] is that it uses reference / anchor points for what climate is.\n",
    "\n",
    "#Currently, the prompts are NOT based on actual data, but simply ChatGPT generated prompts. Change accordingly and discuss with Luca  \n",
    "climate_prompts = [\"The world is heating up\", \"Global warming is imminent\", \n",
    "                   \"Marine life is dwindling\" , \"The seas are rising\", \n",
    "                   \"Carbon emissions are reaching a record high\", \"The Climate crisis is a hoax\" \n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02835135 -0.00131723  0.03569233 ... -0.10806449 -0.06337425\n",
      "   0.03882696]\n",
      " [-0.07531293  0.02271377  0.08144233 ... -0.06254279 -0.01569482\n",
      "  -0.00166639]\n",
      " [-0.029637    0.03345019  0.0721257  ... -0.08058383  0.00560287\n",
      "   0.08174925]\n",
      " [-0.05719218  0.0022417   0.0710148  ... -0.05447808 -0.02128065\n",
      "   0.10292619]\n",
      " [ 0.02706154  0.00828526  0.03643303 ... -0.12502515 -0.02559752\n",
      "   0.0579204 ]\n",
      " [-0.01609303  0.02864322  0.07125508 ... -0.03571245 -0.04944063\n",
      "   0.02691268]]\n"
     ]
    }
   ],
   "source": [
    "anchor_embeddings = model.encode(climate_prompts)\n",
    "print(anchor_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of the 150,000 english posts\n",
    "\n",
    "subset_english_posts = english_posts[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 2 climate-related posts out of 15000\n"
     ]
    }
   ],
   "source": [
    "#Takes approx 8.5 mins for 15000 posts\n",
    "\n",
    "\n",
    "threshold = 0.6 #Test this out: the lower --> \n",
    "\n",
    "climate_related_posts = []\n",
    "\n",
    "for post in subset_english_posts:\n",
    "    text = post.get(\"text\", \"\")\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    post_embedding = model.encode([text])[0]\n",
    "    similarities = cosine_similarity([post_embedding], anchor_embeddings)[0]\n",
    "\n",
    "    if np.max(similarities) > threshold:\n",
    "        climate_related_posts.append(post)\n",
    "\n",
    "print(f\"Identified {len(climate_related_posts)} climate-related posts out of {len(subset_english_posts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/Users/tobiasmichelsen/Bachelor_Project/DS_BachelorProject_PH/data/filtered/climate_posts.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(climate_related_posts, f, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape english posts over a week --> \"Randomly\" sample climate-related posts and add them to the anchor points --> Find climate_related_posts and further tune the threshold  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See code from huggingface: If we want more transparency \n",
    "\"\"\"from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Feb  3 2024, 15:58:27) \n[Clang 15.0.0 (clang-1500.3.9.4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
